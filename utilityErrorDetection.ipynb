{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "# import pycaret\n",
    "import datetime as dt\n",
    "import json\n",
    "from sodapy import Socrata\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC Data from City of New York\n",
    "https://data.cityofnewyork.us/Housing-Development/Electric-Consumption-And-Cost-2010-Feb-2022-/jr24-e7cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"jr24-e7cr\", limit=407031)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "results_df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'results_df'\n"
     ]
    }
   ],
   "source": [
    "# %store results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_df.copy()\n",
    "\n",
    "# change data types to match\n",
    "df['consumption_kwh'] = df['consumption_kwh'].astype(float)\n",
    "df['kwh_charges'] = df['kwh_charges'].astype(float)\n",
    "df['days'] = df['days'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only last 5 complete years of data\n",
    "df['revenue_month'] = pd.to_datetime(df['revenue_month'], format = '%Y-%m')\n",
    "df = df.loc[(df['revenue_month']>='2017-01-01' )& (df['revenue_month']<'2022-01-01')]\n",
    "\n",
    "# take only month integer\n",
    "df['revenue_month'] = df['revenue_month'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "development_name           0\n",
       "borough                    0\n",
       "account_name               0\n",
       "location                4172\n",
       "meter_amr                154\n",
       "tds                     1055\n",
       "edp                        0\n",
       "rc_code                    0\n",
       "funding_source             0\n",
       "amp                      677\n",
       "vendor_name                0\n",
       "umis_bill_id               0\n",
       "revenue_month              0\n",
       "service_start_date         8\n",
       "service_end_date           8\n",
       "days                       8\n",
       "meter_number               0\n",
       "estimated                  0\n",
       "current_charges            0\n",
       "rate_class                 0\n",
       "bill_analyzed              0\n",
       "consumption_kwh            0\n",
       "kwh_charges                0\n",
       "consumption_kw             0\n",
       "kw_charges                 0\n",
       "other_charges              0\n",
       "meter_scope           169785\n",
       "anom_cost                  0\n",
       "anom_qty                   0\n",
       "has_cost_anom              0\n",
       "has_qty_anom               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of missing values for every column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "development_name         336\n",
       "borough                    7\n",
       "account_name             354\n",
       "location                 488\n",
       "meter_amr                  6\n",
       "tds                      336\n",
       "edp                      331\n",
       "rc_code                  385\n",
       "funding_source             7\n",
       "amp                      158\n",
       "vendor_name                3\n",
       "umis_bill_id           49414\n",
       "revenue_month             12\n",
       "service_start_date       299\n",
       "service_end_date         327\n",
       "days                      47\n",
       "meter_number            6131\n",
       "estimated                  3\n",
       "current_charges       136297\n",
       "rate_class                14\n",
       "bill_analyzed              2\n",
       "consumption_kwh         5098\n",
       "kwh_charges            56607\n",
       "consumption_kw          9929\n",
       "kw_charges             50707\n",
       "other_charges         110171\n",
       "meter_scope              100\n",
       "anom_cost              60157\n",
       "anom_qty                7428\n",
       "has_cost_anom              2\n",
       "has_qty_anom               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of all unique values for every column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/1795570298.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['meter_amr'] = df['meter_amr'].str.replace('ENT|B-12|398|NONE', 'OTHER/NONE')\n"
     ]
    }
   ],
   "source": [
    "df['meter_amr'] = df['meter_amr'].str.replace('ENT|B-12|398|NONE', 'OTHER/NONE')\n",
    "df['meter_amr'] = np.where(df['meter_amr'].isna(), 'OTHER/NONE', df['meter_amr'] )\n",
    "df['funding_source'] =  np.where(df['funding_source']!='FEDERAL', 'OTHER', df['funding_source'])\n",
    "df['rate_class'] = np.where(df['rate_class'].str.contains('GOV/NYC'), df['rate_class'], 'OTHER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add anomalies to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-synthetic-time-series-with-anomaly-signatures-in-python-c0b80a6c093c\n",
    "\n",
    "As a synthetic data generation method, you want to control the following characteristics of the anomalies:\n",
    "Fraction of the data that need to be anomalous\n",
    "The scale of the anomaly (how far they lie from the normal)\n",
    "One-sided or two-sided (higher or lower than the normal data in magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Anomalies: 10905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat1_cost] = df['anom_cost'].iloc[cat1_cost] * 10\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat2_cost] = df['anom_cost'].iloc[cat2_cost] * 100\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat3_cost] = df['anom_cost'].iloc[cat3_cost] * .1\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat4_cost] = df['anom_cost'].iloc[cat4_cost] * .01\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat1_qty] = df['anom_qty'].iloc[cat1_qty] * 10\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat2_qty] = df['anom_qty'].iloc[cat2_qty] * 100\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat3_qty] = df['anom_qty'].iloc[cat3_qty] * .1\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/3722355204.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat4_qty] = df['anom_qty'].iloc[cat4_qty] * .01\n"
     ]
    }
   ],
   "source": [
    "# select random rows to be anomalized - generate list of all indices and pick randomly from list\n",
    "totalRows = len(df)\n",
    "inds = range(totalRows)\n",
    "perc = .05 # 5% of data chosen to be anomalized for both consumption and cost columns\n",
    "n = totalRows * perc\n",
    "\n",
    "\n",
    "# cost indices\n",
    "anomalized_inds_cost = random.sample(inds, int(n))\n",
    "\n",
    "# common typos to add to data - these also take into account adding/removing 0s by accident\n",
    "\n",
    "cat1_cost = anomalized_inds_cost[0::4] # rows with x 10 (one decimal to the right)\n",
    "cat2_cost = anomalized_inds_cost[1::4] # rows with x 100 (2 decimals to the right)\n",
    "cat3_cost = anomalized_inds_cost[2::4] # rows with x .1 (one decimal point to the left)\n",
    "cat4_cost = anomalized_inds_cost[3::4] # rows with x .01 (one decimal point to the left) \n",
    "\n",
    "df['anom_cost'] = df['kwh_charges']\n",
    "df['anom_cost'].iloc[cat1_cost] = df['anom_cost'].iloc[cat1_cost] * 10\n",
    "df['anom_cost'].iloc[cat2_cost] = df['anom_cost'].iloc[cat2_cost] * 100\n",
    "df['anom_cost'].iloc[cat3_cost] = df['anom_cost'].iloc[cat3_cost] * .1\n",
    "df['anom_cost'].iloc[cat4_cost] = df['anom_cost'].iloc[cat4_cost] * .01\n",
    "\n",
    "# qty indices\n",
    "anomalized_inds_qty = random.sample(inds, int(n))\n",
    "\n",
    "cat1_qty = anomalized_inds_qty[0::4] # rows with x 10 (one decimal to the right)\n",
    "cat2_qty = anomalized_inds_qty[1::4] # rows with x 100 (2 decimals to the right)\n",
    "cat3_qty = anomalized_inds_qty[2::4] # rows with x .1 (one decimal point to the left)\n",
    "cat4_qty = anomalized_inds_qty[3::4] # rows with x .01 (one decimal point to the left) \n",
    "\n",
    "df['anom_qty'] = df['consumption_kwh']\n",
    "df['anom_qty'].iloc[cat1_qty] = df['anom_qty'].iloc[cat1_qty] * 10\n",
    "df['anom_qty'].iloc[cat2_qty] = df['anom_qty'].iloc[cat2_qty] * 100\n",
    "df['anom_qty'].iloc[cat3_qty] = df['anom_qty'].iloc[cat3_qty] * .1\n",
    "df['anom_qty'].iloc[cat4_qty] = df['anom_qty'].iloc[cat4_qty] * .01\n",
    "\n",
    "df['has_cost_anom'] = ''\n",
    "df['has_cost_anom'] = np.where((df['anom_cost']!=df['kwh_charges']),'ANOMALY','NO ANOMALY')\n",
    "\n",
    "\n",
    "df['has_qty_anom'] = ''\n",
    "df['has_qty_anom'] = np.where((df['anom_qty']!=df['consumption_kwh']),'ANOMALY','NO ANOMALY')\n",
    "\n",
    "print('Total Anomalies:',len(df.loc[(df['has_qty_anom']=='ANOMALY')|(df['has_cost_anom']=='ANOMALY')]))\n",
    "\n",
    "del cat1_cost\n",
    "del cat2_cost\n",
    "del cat3_cost\n",
    "del cat4_cost\n",
    "\n",
    "del cat1_qty\n",
    "del cat2_qty\n",
    "del cat3_qty\n",
    "del cat4_qty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['borough', 'funding_source', 'revenue_month', 'anom_cost', 'anom_qty','meter_amr', 'rate_class']]\n",
    "X = pd.get_dummies(X, columns=['funding_source', 'borough', 'meter_amr','rate_class'], prefix= ['funding', 'borough', 'meter_amr', 'rate'])\n",
    "\n",
    "\n",
    "scale= StandardScaler()\n",
    " \n",
    "# standardization of dependent variables\n",
    "X = scale.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF predictions complete\n",
      "Iso Forest predictions complete\n",
      "LOF Model Accuracy: 96.0 %  in  9.6 s \n",
      " Iso Forest Model Accuracy: 90.2 %  in  9.6 s\n"
     ]
    }
   ],
   "source": [
    "lof  = LocalOutlierFactor(contamination=.05)\n",
    "iso = IsolationForest(contamination=.05, random_state = 0)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "df['anomaly_outcome_lof'] = lof.fit_predict(X)\n",
    "df['lof_anomaly_score'] = lof.negative_outlier_factor_\n",
    "# df['lof_anomaly_score_radius'] = (df['lof_anomaly_score'].max() - df['lof_anomaly_score']) / (df['lof_anomaly_score'].max() - df['lof_anomaly_score'].min())\n",
    "print('LOF predictions complete')\n",
    "\n",
    "start = time.time()\n",
    "df['anomaly_outcome_iso'] = iso.fit_predict(X)\n",
    "df['iso_anomaly_score'] = abs(iso.score_samples(X))\n",
    "print('Iso Forest predictions complete')\n",
    "\n",
    "\n",
    "df['correct_prediction_lof'] = np.where(((df['anomaly_outcome_lof'] == -1) & ((df['has_qty_anom']=='ANOMALY')|(df['has_cost_anom']=='ANOMALY')))|(df['anomaly_outcome_lof'] == 1) & ((df['has_qty_anom']=='NO ANOMALY')& (df['has_cost_anom']=='NO ANOMALY')), 'YES', 'NO')\n",
    "pred_results_lof = df['correct_prediction_lof'].value_counts()\n",
    "\n",
    "df['correct_prediction_iso'] = np.where(((df['anomaly_outcome_iso'] == -1) & ((df['has_qty_anom']=='ANOMALY')|(df['has_cost_anom']=='ANOMALY')))|(df['anomaly_outcome_iso'] == 1) & ((df['has_qty_anom']=='NO ANOMALY')& (df['has_cost_anom']=='NO ANOMALY')), 'YES', 'NO')\n",
    "pred_results_iso = df['correct_prediction_iso'].value_counts()\n",
    "\n",
    "print('LOF Model Accuracy:',round(pred_results_lof[0]/(pred_results_lof[1]+pred_results_lof[0])*100,1),'%',' in ', round(time.time() - start,1), 's', '\\n', 'Iso Forest Model Accuracy:',round(pred_results_iso[0]/(pred_results_iso[1]+pred_results_iso[0])*100,1),'%', ' in ', round(time.time() - start,1), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/viviantran/projects/jpl_interview/data/processed'\n",
    "os.chdir(path)\n",
    "\n",
    "df.to_csv('modelResults.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Algorithms to Choose From\n",
    "\n",
    "https://towardsdatascience.com/5-anomaly-detection-algorithms-every-data-scientist-should-know-b36c3605ea16\n",
    "Depends on what kind of anomalies exist in our data:\n",
    "- Outliers: Short/small anomalous patterns that appear in a non-systematic way in data collection.\n",
    "- Change in Events: Systematic or sudden change from the previous normal behavior.\n",
    "- Drifts: Slow, undirectional, long-term change in the data.\n",
    "\n",
    "\n",
    "\n",
    "Point anomalies – if a data point is too far from the rest, it falls into the category of point anomalies. The above example of bank transaction illustrates point anomalies.\n",
    "Contextual anomalies – If the event is anomalous for specific circumstances (context), then we have contextual anomalies. As data becomes more and more complex, it is vital to use anomaly detection methods for the context. This anomaly type is common in time-series data. Example – spending $10 on ice-cream every day during the hot months is normal, but is odd for the rest months.\n",
    "Collective anomalies. The collective anomaly denotes a collection of anomalous with respect to the whole dataset, but not individual objects. Example: breaking rhythm in ECG (Electrocardiogram).\n",
    "\n",
    "\n",
    "\n",
    "We are not using simple statistical methods like comparing mean and median because our data is not univariate. Typos/errors can occur in the cost or consumption columns, so we need to choose a method that can take this into account. In addition, natural dips and peaks that are typical for that account could be marked as outliers if we were to do simple comparisons. A machine learning approach will take into account the historical context. \n",
    "\n",
    "Simple statistical techniques such as mean, median, quantiles can be used to detect univariate anomalies feature values in the dataset. Various data visualization and exploratory data analysis techniques can be also be used to detect anomalies.\n",
    "\n",
    "This project originated from my work with naval utility data, which contained electricity, water, natural gas, and sewer along with over 4000 different accounts and line item descriptions to take into account. Since this data is sensitive, I've recreated the assignment using NYC utility data, which only contains electricity and about 300 accounts. The NYC data is also cleaned an doesn't contain any errors, so I artificially introduced some errors into the data (approx 5% of the data will contain errors). The original dataset where I was working with all naval utilities worldwide had much more errors because we dealt with a lot of manual data entry. We also had bill corrections which this data does not contain. This data comes directly from the electricity meter. \n",
    "\n",
    "\n",
    "\n",
    "Checklist:\n",
    "1. Isolation Forest\n",
    "   1. Chose isolation forest because it is easy to interpret for our stakeholders\n",
    "2. Local Outlier Factor\n",
    "3. Robust Covariance\n",
    "4. One-Class SVM\n",
    "5. One-Class SVM (SGD)\n",
    "\n",
    "https://www.intellspot.com/anomaly-detection-algorithms/\n",
    "\n",
    "More Anomaly Info:\n",
    "https://serokell.io/blog/anomaly-detection-in-machine-learning\n",
    "- We are dealing with contextual outliers\n",
    "\n",
    "Requirements:\n",
    "- Unsupervised: don't actually know what is is/isnt an error\n",
    "- Flexible/robust enough to account for data changes\n",
    "- Interpretable\n",
    "\n",
    "Looking at only variance and standard deviations assumes that all groups will behave the same and doesn't take into account context of the account\n",
    "Rule based analysis where we are looking at vaue thresholds assume we know what an acceptable range is. What happens when we have new data? How we take that into account? Do we have to create rules for every single group? Unwieldy for large datasets and not sustainable in the long-run\n",
    "\n",
    "The k-NN algorithm works very well for dynamic environments where frequent updates are needed. In addition, density-based distance measures are good solutions for identifying unusual conditions and gradual trends. This makes k-NN useful for outlier detection and defining suspicious events.\n",
    "k-NN also is very good techniques for creating models that involve non-standard data types like text.\n",
    "\n",
    "k-NN is one of the proven anomaly detection algorithms that increase the fraud detection rate. It is also one of the most known text mining algorithms out there.\n",
    "\n",
    "\n",
    "The LOF is a key anomaly detection algorithm based on a concept of a local density. It uses the distance between the k nearest neighbors to estimate the density.\n",
    "\n",
    "LOF compares the local density of an item to the local densities of its neighbors. Thus one can determine areas of similar density and items that have a significantly lower density than their neighbors. These are the outliers.\n",
    "\n",
    "\n",
    "K-means clustering\n",
    "- Only works with numeric data - we have some categorical cols\n",
    "\n",
    "Time series\n",
    "- Not using this because we are taking into account more than just the numeric values. We want to feed in the vendor, location, rate type because anomalies could be specific to groups within these variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf - iso tree paper\n",
    "\n",
    "Anomalies are more susceptible to isolation and hence have short path lengths (used to calculate the anomaly score)\n",
    "\n",
    "calculated iso tree anomaly scores:\n",
    "- Very close to 1 = definitely anomalies\n",
    "- much smaller than 0.5, then definitely normal\n",
    "- all instances ~0.5, then sample doesn't have distinct anomaly\n",
    "\n",
    "\n",
    "LOF\n",
    "raw scores: larger scores = more likely to be outlier\n",
    "LOF measures how isolated a point is from its surrounding neighbors \n",
    "\n",
    "\n",
    "isotree output easier to interpret \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf.negative_outlier_factor_\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@arpitbhayani/isolation-forest-algorithm-for-anomaly-detection-f88af2d5518d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
