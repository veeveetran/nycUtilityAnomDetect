{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection Algorithms to Choose From\n",
    "\n",
    "https://towardsdatascience.com/5-anomaly-detection-algorithms-every-data-scientist-should-know-b36c3605ea16\n",
    "Depends on what kind of anomalies exist in our data:\n",
    "- Outliers: Short/small anomalous patterns that appear in a non-systematic way in data collection.\n",
    "- Change in Events: Systematic or sudden change from the previous normal behavior.\n",
    "- Drifts: Slow, undirectional, long-term change in the data.\n",
    "\n",
    "\n",
    "\n",
    "Point anomalies – if a data point is too far from the rest, it falls into the category of point anomalies. The above example of bank transaction illustrates point anomalies.\n",
    "Contextual anomalies – If the event is anomalous for specific circumstances (context), then we have contextual anomalies. As data becomes more and more complex, it is vital to use anomaly detection methods for the context. This anomaly type is common in time-series data. Example – spending $10 on ice-cream every day during the hot months is normal, but is odd for the rest months.\n",
    "Collective anomalies. The collective anomaly denotes a collection of anomalous with respect to the whole dataset, but not individual objects. Example: breaking rhythm in ECG (Electrocardiogram).\n",
    "\n",
    "\n",
    "\n",
    "We are not using simple statistical methods like comparing mean and median because our data is not univariate. Typos/errors can occur in the cost or consumption columns, so we need to choose a method that can take this into account. In addition, natural dips and peaks that are typical for that account could be marked as outliers if we were to do simple comparisons. A machine learning approach will take into account the historical context. \n",
    "\n",
    "Simple statistical techniques such as mean, median, quantiles can be used to detect univariate anomalies feature values in the dataset. Various data visualization and exploratory data analysis techniques can be also be used to detect anomalies.\n",
    "\n",
    "This project originated from my work with naval utility data, which contained electricity, water, natural gas, and sewer along with over 4000 different accounts and line item descriptions to take into account. Since this data is sensitive, I've recreated the assignment using NYC utility data, which only contains electricity and about 300 accounts. The NYC data is also cleaned an doesn't contain any errors, so I artificially introduced some errors into the data (approx 5% of the data will contain errors). The original dataset where I was working with all naval utilities worldwide had much more errors because we dealt with a lot of manual data entry. We also had bill corrections which this data does not contain. This data comes directly from the electricity meter. \n",
    "\n",
    "\n",
    "\n",
    "Checklist:\n",
    "1. Isolation Forest\n",
    "   1. Chose isolation forest because it is easy to interpret for our stakeholders\n",
    "2. Local Outlier Factor\n",
    "3. Robust Covariance\n",
    "4. One-Class SVM\n",
    "5. One-Class SVM (SGD)\n",
    "\n",
    "https://www.intellspot.com/anomaly-detection-algorithms/\n",
    "\n",
    "More Anomaly Info:\n",
    "https://serokell.io/blog/anomaly-detection-in-machine-learning\n",
    "- We are dealing with contextual outliers\n",
    "\n",
    "Requirements:\n",
    "- Unsupervised: don't actually know what is is/isnt an error\n",
    "- Flexible/robust enough to account for data changes\n",
    "- Interpretable\n",
    "\n",
    "Looking at only variance and standard deviations assumes that all groups will behave the same and doesn't take into account context of the account\n",
    "Rule based analysis where we are looking at vaue thresholds assume we know what an acceptable range is. What happens when we have new data? How we take that into account? Do we have to create rules for every single group? Unwieldy for large datasets and not sustainable in the long-run\n",
    "\n",
    "The k-NN algorithm works very well for dynamic environments where frequent updates are needed. In addition, density-based distance measures are good solutions for identifying unusual conditions and gradual trends. This makes k-NN useful for outlier detection and defining suspicious events.\n",
    "k-NN also is very good techniques for creating models that involve non-standard data types like text.\n",
    "\n",
    "k-NN is one of the proven anomaly detection algorithms that increase the fraud detection rate. It is also one of the most known text mining algorithms out there.\n",
    "\n",
    "\n",
    "The LOF is a key anomaly detection algorithm based on a concept of a local density. It uses the distance between the k nearest neighbors to estimate the density.\n",
    "\n",
    "LOF compares the local density of an item to the local densities of its neighbors. Thus one can determine areas of similar density and items that have a significantly lower density than their neighbors. These are the outliers.\n",
    "\n",
    "\n",
    "K-means clustering\n",
    "- Only works with numeric data - we have some categorical cols\n",
    "\n",
    "Time series\n",
    "- Not using this because we are taking into account more than just the numeric values. We want to feed in the vendor, location, rate type because anomalies could be specific to groups within these variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "# import pycaret\n",
    "import datetime as dt\n",
    "import json\n",
    "from sodapy import Socrata\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go with LOF or isolation forest because of high dimensional data - robust, flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC Data from City of New York\n",
    "https://data.cityofnewyork.us/Housing-Development/Electric-Consumption-And-Cost-2010-Feb-2022-/jr24-e7cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"jr24-e7cr\", limit=407031)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "results_df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'results_df'\n"
     ]
    }
   ],
   "source": [
    "# %store results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop accounts with less than 8 years of data\n",
    "df = results_df.groupby('account_name').filter(lambda x: len(x) >= 96)\n",
    "\n",
    "df['consumption_kwh'] = df['consumption_kwh'].astype(float)\n",
    "df['kwh_charges'] = df['kwh_charges'].astype(float)\n",
    "df['days'] = df['days'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only last 5 years of data\n",
    "df['revenue_month'] = pd.to_datetime(df['revenue_month'], format = '%Y-%m')\n",
    "\n",
    "\n",
    "# take only month integer\n",
    "df['revenue_month'] = df['revenue_month'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kwh_charges</th>\n",
       "      <th>anom_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>990.98</td>\n",
       "      <td>9909.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2585.83</td>\n",
       "      <td>25858.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>885.68</td>\n",
       "      <td>88.5680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>105.39</td>\n",
       "      <td>1053.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>820.65</td>\n",
       "      <td>82.0650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406960</th>\n",
       "      <td>2908.15</td>\n",
       "      <td>29081.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406990</th>\n",
       "      <td>180.83</td>\n",
       "      <td>18083.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407020</th>\n",
       "      <td>421.83</td>\n",
       "      <td>42183.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407023</th>\n",
       "      <td>23.76</td>\n",
       "      <td>0.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407027</th>\n",
       "      <td>296.36</td>\n",
       "      <td>29.6360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        kwh_charges   anom_cost\n",
       "2            990.98   9909.8000\n",
       "9           2585.83  25858.3000\n",
       "16           885.68     88.5680\n",
       "79           105.39   1053.9000\n",
       "112          820.65     82.0650\n",
       "...             ...         ...\n",
       "406960      2908.15  29081.5000\n",
       "406990       180.83  18083.0000\n",
       "407020       421.83  42183.0000\n",
       "407023        23.76      0.2376\n",
       "407027       296.36     29.6360\n",
       "\n",
       "[13455 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['has_cost_anom']=='ANOMALY'][['kwh_charges','anom_cost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add anomalies to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-synthetic-time-series-with-anomaly-signatures-in-python-c0b80a6c093c\n",
    "\n",
    "As a synthetic data generation method, you want to control the following characteristics of the anomalies:\n",
    "Fraction of the data that need to be anomalous\n",
    "The scale of the anomaly (how far they lie from the normal)\n",
    "One-sided or two-sided (higher or lower than the normal data in magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat1_cost] = df['anom_cost'].iloc[cat1_cost] * 10\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat2_cost] = df['anom_cost'].iloc[cat2_cost] * 100\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat3_cost] = df['anom_cost'].iloc[cat3_cost] * .1\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_cost'].iloc[cat4_cost] = df['anom_cost'].iloc[cat4_cost] * .01\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat1_qty] = df['anom_qty'].iloc[cat1_qty] * 10\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat2_qty] = df['anom_qty'].iloc[cat2_qty] * 100\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat3_qty] = df['anom_qty'].iloc[cat3_qty] * .1\n",
      "/var/folders/hp/1lw7zlt10ql7hhc7hv8cm8lr0000gn/T/ipykernel_34901/2004268458.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anom_qty'].iloc[cat4_qty] = df['anom_qty'].iloc[cat4_qty] * .01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NO ANOMALY    392621\n",
       "ANOMALY        13415\n",
       "Name: has_qty_anom, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select random rows to be anomalized - generate list of all indices and pick randomly from list\n",
    "totalRows = len(df)\n",
    "inds = range(totalRows)\n",
    "perc = .05 # 5% of data chosen to be anomalized for both consumption and cost columns\n",
    "n = totalRows * perc\n",
    "\n",
    "\n",
    "# cost indices\n",
    "anomalized_inds_cost = random.sample(inds, int(n))\n",
    "\n",
    "# common typos to add to data - these also take into account adding/removing 0s by accident\n",
    "\n",
    "cat1_cost = anomalized_inds_cost[0::4] # rows with x 10 (one decimal to the right)\n",
    "cat2_cost = anomalized_inds_cost[1::4] # rows with x 100 (2 decimals to the right)\n",
    "cat3_cost = anomalized_inds_cost[2::4] # rows with x .1 (one decimal point to the left)\n",
    "cat4_cost = anomalized_inds_cost[3::4] # rows with x .01 (one decimal point to the left) \n",
    "\n",
    "df['anom_cost'] = df['kwh_charges']\n",
    "df['anom_cost'].iloc[cat1_cost] = df['anom_cost'].iloc[cat1_cost] * 10\n",
    "df['anom_cost'].iloc[cat2_cost] = df['anom_cost'].iloc[cat2_cost] * 100\n",
    "df['anom_cost'].iloc[cat3_cost] = df['anom_cost'].iloc[cat3_cost] * .1\n",
    "df['anom_cost'].iloc[cat4_cost] = df['anom_cost'].iloc[cat4_cost] * .01\n",
    "\n",
    "# qty indices\n",
    "anomalized_inds_qty = random.sample(inds, int(n))\n",
    "\n",
    "cat1_qty = anomalized_inds_qty[0::4] # rows with x 10 (one decimal to the right)\n",
    "cat2_qty = anomalized_inds_qty[1::4] # rows with x 100 (2 decimals to the right)\n",
    "cat3_qty = anomalized_inds_qty[2::4] # rows with x .1 (one decimal point to the left)\n",
    "cat4_qty = anomalized_inds_qty[3::4] # rows with x .01 (one decimal point to the left) \n",
    "\n",
    "df['anom_qty'] = df['consumption_kwh']\n",
    "df['anom_qty'].iloc[cat1_qty] = df['anom_qty'].iloc[cat1_qty] * 10\n",
    "df['anom_qty'].iloc[cat2_qty] = df['anom_qty'].iloc[cat2_qty] * 100\n",
    "df['anom_qty'].iloc[cat3_qty] = df['anom_qty'].iloc[cat3_qty] * .1\n",
    "df['anom_qty'].iloc[cat4_qty] = df['anom_qty'].iloc[cat4_qty] * .01\n",
    "\n",
    "df['has_cost_anom'] = ''\n",
    "df['has_cost_anom'] = np.where((df['anom_cost']!=df['kwh_charges']),'ANOMALY','NO ANOMALY')\n",
    "df['has_cost_anom'].value_counts()\n",
    "\n",
    "df['has_qty_anom'] = ''\n",
    "df['has_qty_anom'] = np.where((df['anom_qty']!=df['consumption_kwh']),'ANOMALY','NO ANOMALY')\n",
    "df['has_qty_anom'].value_counts()\n",
    "\n",
    "\n",
    "del cat1_cost\n",
    "del cat2_cost\n",
    "del cat3_cost\n",
    "del cat4_cost\n",
    "\n",
    "del cat1_qty\n",
    "del cat2_qty\n",
    "del cat3_qty\n",
    "del cat4_qty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df.groupby('account_name').agg({lambda x: list(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['borough', 'funding_source', 'revenue_month', 'anom_cost', 'anom_qty', 'rate_class']]\n",
    "X = pd.get_dummies(X, columns=['rate_class', 'funding_source', 'borough'], prefix= ['rate_class', 'funding', 'borough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revenue_month                                  int64\n",
       "anom_cost                                    float64\n",
       "anom_qty                                     float64\n",
       "rate_class_281-Sec Com Large Gen Use           uint8\n",
       "rate_class_284-Sec Com Large Multiple Per      uint8\n",
       "rate_class_285-Prim Com Large Mult Per         uint8\n",
       "rate_class_285-Sec Com Large Multi Period      uint8\n",
       "rate_class_68                                  uint8\n",
       "rate_class_EL2                                 uint8\n",
       "rate_class_EL2 Small Non-Res                   uint8\n",
       "rate_class_EL9                                 uint8\n",
       "rate_class_GOV/NYC/062                         uint8\n",
       "rate_class_GOV/NYC/064                         uint8\n",
       "rate_class_GOV/NYC/068                         uint8\n",
       "rate_class_GOV/NYC/068 HT                      uint8\n",
       "rate_class_GOV/NYC/068 TOD                     uint8\n",
       "rate_class_GOV/NYC/069                         uint8\n",
       "rate_class_GOV/NYC/069 TOD                     uint8\n",
       "rate_class_GOV/NYC/082                         uint8\n",
       "rate_class_Rate 285                            uint8\n",
       "funding_FEDERAL                                uint8\n",
       "funding_FEDERAL-COOP                           uint8\n",
       "funding_MIXED FINANCE/LLC1                     uint8\n",
       "funding_MIXED FINANCE/LLC2                     uint8\n",
       "funding_NON-DEVELOPMENT                        uint8\n",
       "funding_SECTION 8                              uint8\n",
       "borough_BRONX                                  uint8\n",
       "borough_BROOKLYN                               uint8\n",
       "borough_FHA                                    uint8\n",
       "borough_MANHATTAN                              uint8\n",
       "borough_NON DEVELOPMENT FACILITY               uint8\n",
       "borough_QUEENS                                 uint8\n",
       "borough_STATEN ISLAND                          uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "\n",
    "for i in range(len(list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor()\n",
    "\n",
    "\n",
    "df['anomaly_score'] = 0.0\n",
    "df['anomaly_score'] = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    374529\n",
       "-1     31507\n",
       "Name: anomaly_score, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['anomaly_score'].value_counts(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_qty_anom  has_cost_anom\n",
       "NO ANOMALY    NO ANOMALY       379810\n",
       "              ANOMALY           12811\n",
       "ANOMALY       NO ANOMALY        12771\n",
       "              ANOMALY             644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['has_qty_anom', 'has_cost_anom']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/viviantran/projects/jpl_interview/data/processed'\n",
    "os.chdir(path)\n",
    "\n",
    "modelName = 'lof'\n",
    "df.to_csv('results_'+modelName+'.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
